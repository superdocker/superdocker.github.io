---
---

@inproceedings{
iceic2024searching,
abbr={ICEIC 2024},
abstract_unready={},
pdf_unready={},
title={Searching Optimal Floating-Point Format for Sub-8-Bit Large Language Model Inference},
author={Youngdeok Hwang* and Janghwan Lee* and Jiwoong Park and Jieun Lim and Jungwook Choi},
booktitle={International Conference on Electronics, Information, and Communication (ICEIC)},
year={2024},
url={},
}

@inproceedings{
anonymous2023spade,
abbr={HPCA 2024},
abstract_unready={},
pdf_unready={},
title={SPADE: Sparse Pillar-based 3D Object Detection Accelerator for Autonomous Driving},
author={Minjae Lee and Seongmin Park and Hyungmin Kim and Minyong Yoon and Janghwan Lee and Junwon Choi and Nam Sung Kim and Mingu Kang and Jungwook Choi},
booktitle={30th IEEE International Symposium on High-Performance Computer Architecture (HPCA 2024)},
year={2024},
url={},
  selected={true}
}

@inproceedings{
lee-etal-2023-enhancing-computation,
abbr={EMNLP 2023},
abstract={Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency—a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths to target tasks. Moreover, we introduce dINT, a hybrid data format combining integer and denormal representations, to address the underflow issue in W4A8 quantization, where small values are rounded to zero. Through rigorous evaluations of LLMs, including OPT and LLaMA, we demonstrate that our techniques significantly boost task accuracies to levels comparable with full-precision models. By developing arithmetic units compatible with dINT, we further confirm that our methods yield a 2× hardware efficiency improvement compared to 8-bit integer MAC unit.},
pdf={https://aclanthology.org/2023.emnlp-main.910.pdf},
title={Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization},
author={Janghwan Lee* and Minsoo Kim* and Seungcheol Baek and Seokjoong Hwang and Wonyong Sung and Jungwook Choi},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=4Ggw1DsgRQ},
  selected={true}
}

@inproceedings{
anonymous2023tokenscaled,
abbr={NeurIPS 2023},
abstract_unready={},
pdf_unready={},
title={Token-Scaled Logit Distillation for Ternary Weight Generative Language Models},
author={Minsoo Kim and Sihwa Lee and Janghwan Lee and Sukjin Hong and Duseong Chang and Wonyong Sung and Jungwook Choi},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=FUnEkOkodU},
  selected={true}
}

@misc{lee2023pillaracc,
abbr={arXiv 2023},
abstract={3D object detection using point cloud (PC) data is vital for autonomous driving perception pipelines, where efficient encoding is key to meeting stringent resource and latency requirements. PointPillars, a widely adopted bird's-eye view (BEV) encoding, aggregates 3D point cloud data into 2D pillars for high-accuracy 3D object detection. However, most state-of-the-art methods employing PointPillar overlook the inherent sparsity of pillar encoding, missing opportunities for significant computational reduction. In this study, we propose a groundbreaking algorithm-hardware co-design that accelerates sparse convolution processing and maximizes sparsity utilization in pillar-based 3D object detection networks. We investigate sparsification opportunities using an advanced pillar-pruning method, achieving an optimal balance between accuracy and sparsity. We introduce PillarAcc, a state-of-the-art sparsity support mechanism that enhances sparse pillar convolution through linear complexity input-output mapping generation and conflict-free gather-scatter memory access. Additionally, we propose dataflow optimization techniques, dynamically adjusting the pillar processing schedule for optimal hardware utilization under diverse sparsity operations. We evaluate PillarAcc on various cutting-edge 3D object detection networks and benchmarks, achieving remarkable speedup and energy savings compared to representative edge platforms, demonstrating record-breaking PointPillars speed of 500FPS with minimal compromise in accuracy.},
pdf={https://arxiv.org/abs/2305.07522},
title={PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices}, 
author={Minjae Lee and Hyungmin Kim and Seongmin Park and Minyong Yoon and Janghwan Lee and Junwon Choi and Mingu Kang and Jungwook Choi},
year={2023},
eprint={2305.07522},
archivePrefix={arXiv},
primaryClass={cs.AR}
}

@INPROCEEDINGS{10247958,
abbr={DAC 2023},
author={Kim, Janghyeon and Lee, Janghwan and Han, JeongHo and Lee, Sangheon and Choi, Jungwook},
pdf={https://ieeexplore.ieee.org/abstract/document/10247958},
abstract={This paper proposes a range-invariant approximation of non-linear operations for training computations of Transformer-based large language models. The proposed method decomposes the approximation into the scaling and the range-invariant resolution for LUT approximation, covering diverse data ranges of non-linear operations with drastically reduced LUT entries during task-dependent BERT fine-tuning. We demonstrate that the proposed method robustly approximates all the non-linear operations of BERT without score degradation on challenging GLUE benchmarks using only a single-entry LUT, facilitating 52\% area savings in hardware implementation.},
booktitle={2023 60th ACM/IEEE Design Automation Conference (DAC)}, 
title={Range-Invariant Approximation of Non-Linear Operations for Efficient BERT Fine-Tuning}, 
year={2023},
volume={},
number={},
pages={1-6},
doi={10.1109/DAC56929.2023.10247958},
selected={true}
}

@INPROCEEDINGS{10096798,
abbr={ICASSP 2023},
author={Lee, Janghwan and Hwang, Youngdeok and Choi, Jungwook},
pdf={https://ieeexplore.ieee.org/document/10096798},
abstract={Vision Transformers (ViTs) have gained significant attention for their exceptional model accuracies on computer vision applications, but their demanding memory requirements and computational complexity have hindered active deployment. Post-training quantization (PTQ) is a practical method to tackle this challenge by directly reducing ViT’s bit-precision. However, diverse data characteristics across different operations of ViT cannot be well captured solely by a single numerical format (fixed or floating-point). This work proposes an analytical framework that optimizes the numerical format of each matrix multiplication of ViTs for mixed-format sub-8bit quantization. The extensive evaluation demonstrates that the proposed method can reduce the PTQ error and achieve state-of-the-art accuracy for popular ViT models.},
booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Finding Optimal Numerical Format for Sub-8-Bit Post-Training Quantization of Vision Transformers}, 
year={2023},
volume={},
number={},
pages={1-5},
doi={10.1109/ICASSP49357.2023.10096798},
selected={true}
}

@INPROCEEDINGS{9869965,
abbr={AICAS 2022},
author={Lee, Janghwan and Choi, Jungwook},
pdf={https://ieeexplore.ieee.org/document/9869965},
abstract={The Transformer-based fine-tuned neural networks have demonstrated remarkable success in natural language processing (NLP) at the cost of a substantial computational burden. Post-training quantization (PTQ) is a promising technique to reduce the computational cost without expensive re-training. But prior works either demand complex calibration or suffer noticeable accuracy degradation. This paper proposes a practical method for sub-8bit floating-point (FP) PTQ. The proposed method optimizes the exponent bias to minimize quantization error in terms of signal-to-quantization noise ratio (SQNR) progressively like stochastic gradient descent. We evaluate that the proposed method achieves close to full-precision model accuracy for 6 to 8 bit FP PTQ of fine-tuned BERT on GLUE and SQuAD tasks with negligible run-time overhead.},
booktitle={2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS)}, 
title={Optimizing Exponent Bias for Sub-8bit Floating-Point Inference of Fine-tuned Transformers}, 
year={2022},
volume={},
number={},
pages={98-101},
doi={10.1109/AICAS54282.2022.9869965}}

